from collections import defaultdict
import os
import pickle as pkl
from pathlib import PurePath
from typing import Any, Dict, List, Optional, Tuple, Union

import numpy as np
from scipy.stats.stats import pearsonr
import torch
import torch.nn.functional as F
import torchvision.transforms as T
from tqdm import tqdm


def load_maps(
    path_to_maps: Union[str, PurePath],
):
    """
    Loads the maps generated by gen_saliency_maps.

    Parameters
    ----------
    path_to_maps : Union[str, PurePath]
        The path to the maps.

    Returns
    -------
    Tuple[Dict[Union[str, PurePath], Tuple[int, np.ndarray, np.ndarray]]]
        A dictionary containing the maps.
    """

    if isinstance(path_to_maps, str):
        path_to_maps = PurePath(path_to_maps)

    maps = dict()
    pbar = tqdm(
        total=os.path.getsize(path_to_maps),
        unit="B",
        unit_scale=True,
        desc="Loading maps to memory"
    )

    with open(path_to_maps, "rb") as f:
        while True:
            try:
                idx, img, map = pkl.load(f)
                maps[int(idx)] = (np.array(img), np.array(map))
            except EOFError:
                break

            pbar.update(f.tell() - pbar.n)

    pbar.close()
    return maps


def load_dauc_iauc(
    path_to_dauc_iauc: Union[str, PurePath],
) -> Dict[str, list]:


    if isinstance(path_to_dauc_iauc, str):
        path_to_dauc_iauc = PurePath(path_to_dauc_iauc)

    dauc_iauc = defaultdict(list)

    pbar = tqdm(
        total=os.path.getsize(path_to_dauc_iauc),
        unit="B",
        unit_scale=True,
        desc="Loading DAUC and IAUC to memory"
    )

    with open(path_to_dauc_iauc, "rb") as f:
        while True:
            try:
                idx, dauc, iauc = pkl.load(f)
                dauc_iauc['iauc'].append(iauc)
                dauc_iauc['dauc'].append(dauc)
                dauc_iauc['idx'].append(idx)
            except EOFError:
                break
            pbar.update(f.tell() - pbar.n)

    return dauc_iauc


def norm_map(
    map: np.ndarray
):
    """
    Normalizes the map.

    Parameters
    ----------
    map : np.ndarray
        The map to normalize.

    Returns
    -------
    np.ndarray
        The normalized map.
    """

    return (map - np.min(map)) / (np.max(map) - np.min(map))


class AUCBase():

    def __init__(
        self,
        model: torch.nn.Module,
        maps: Dict[int, Tuple[np.ndarray, np.ndarray]],
        device: torch.device,
        masking_steps: Optional[int] = None,
    ):
        self.model = model
        self.maps = maps
        self.device = device
        self.masking_steps = masking_steps

        self.keys = list(self.maps.keys())
        self.current_idx = 0

        self.transform = T.Compose([
            T.ToTensor(),
            T.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225],
            )
        ])

        # Set the auc_calc to the appropriate function in the subclass.
        self.auc_calc = self._get_auc

    def __len__(self):
        return len(self.keys)

    def __iter__(self):
        self.current_idx = 0
        return self

    def __next__(self):
        if self.current_idx >= len(self.keys):
            raise StopIteration

        result = self.auc_calc()
        self.current_idx += 1
        return result

    def _get_auc(self):
        raise NotImplementedError


class DAUC(AUCBase):

    def __init__(
        self,
        model: torch.nn.Module,
        maps: Dict[int, Tuple[np.ndarray, np.ndarray]],
        device: torch.device,
        masking_steps: Optional[int] = None,
        verbose: bool = False,
    ):
        super().__init__(
            model=model,
            maps=maps,
            device=device,
            masking_steps=masking_steps,
        )
        self.verbose = verbose
        self.auc_calc = self._get_dauc

    def _get_masked_image(
        self,
        img: np.ndarray,
        saliency_map: np.ndarray,
        mask: Optional[np.ndarray] = None,
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        Applies the mask to the image.

        Parameters
        ----------
        img : np.ndarray
            The image with three dimensions in channel first order
            (channels, height, width).
        saliency_map : np.ndarray
            The saliency map with two dimensions, height and width.
            If the saliency map has three dimensions, the sum over
            the first dimension is calculated. The saliency map
            is then normalized.
        mask : Optional[np.ndarray], optional
            The mask to apply to the image, by default None.
            Can be used to apply a mask from a previous masking step.

        Returns
        -------
        Tuple[np.ndarray, np.ndarray]
            The masked image and the mask.
        """

        if len(saliency_map.shape) == 3:
            saliency_map = norm_map(np.sum(saliency_map, axis=0)).squeeze()
            saliency_map = saliency_map * 255
            saliency_map = saliency_map.astype(np.uint8)

        r = img.shape[1]/saliency_map.shape[0]
        assert r == img.shape[2]/saliency_map.shape[1], \
            "The image and the saliency map must have the same aspect ratio."

        mask = np.ones((img.shape[1], img.shape[2])) if mask is None else mask
        steps = np.count_nonzero(mask == 0)
        steps = int((steps // r**2))

        # get the indices of the sorted saliency map in descending order
        sort_idx = np.unravel_index(
            np.argsort(saliency_map.ravel(), axis=None),
            saliency_map.shape
        )

        for i, j in zip(sort_idx[0][::-1][steps:], sort_idx[1][::-1][steps:]):
            mask[int(i*r):int((i+1)*r), int(j*r):int((j+1)*r)] = 0
            return img * np.repeat(mask[np.newaxis, :, :], 3, axis=0), mask

        return img * 0, mask * 0

    def _get_dauc(self):
        img, map = self.maps[self.keys[self.current_idx]]
        masked_image = img
        mask = np.ones((img.shape[1], img.shape[2]))
        outputs = []

        if len(map.shape) == 3:
            map = norm_map(np.sum(map, axis=0)).squeeze()
            map = map * 255
            map = map.astype(np.uint8)

        if self.verbose:
            pbar = tqdm(
                desc=f'DAUC - Masking image {self.current_idx}',
                total=map.size,
            )

        i = 0
        masked_imgs = []

        while i < map.size:

            masked_image, mask = self._get_masked_image(
                img=img,
                saliency_map=map,
                mask=mask,
            )
            norm_img = self.transform(np.moveaxis(masked_image, 0, -1))
            norm_img = norm_img.type(torch.FloatTensor).unsqueeze(0)
            masked_imgs.append(norm_img)

            if len(masked_imgs) >= 64 or mask.sum() == 0:
                masked_imgs = torch.cat(masked_imgs, dim=0)
                prediction = self.model(masked_imgs.to(self.device))
                outputs.extend(prediction.detach().cpu().tolist())
                masked_imgs = []

            if self.verbose:
                pbar.update(1)
            i += 1

            if self.masking_steps is not None and i >= self.masking_steps:
                if len(masked_imgs) > 0:
                    masked_imgs = torch.cat(masked_imgs, dim=0)
                    prediction = self.model(masked_imgs.to(self.device))
                    outputs.extend(prediction.detach().cpu().tolist())
                break

        if self.verbose:
            pbar.close()
        outputs = np.array(outputs)
        outputs = F.softmax(torch.tensor(outputs), dim=1).numpy()

        inital_prediction = np.argmax(outputs[0])
        deletion_score = outputs[:, inital_prediction]
        deletion_score /= deletion_score.max()

        return (
            np.trapz(deletion_score, dx=1/map.size),
            deletion_score
        )


class IAUC(AUCBase):

    def __init__(
        self,
        model: torch.nn.Module,
        maps: Dict[int, Tuple[np.ndarray, np.ndarray]],
        device: torch.device,
        masking_steps: Optional[int] = None,
        verbose: bool = False,
    ):
        super().__init__(
            model=model,
            maps=maps,
            device=device,
            masking_steps=masking_steps,
        )

        self.verbose = verbose
        self.auc_calc = self._get_iauc

    def _get_masked_image(
        self,
        img: np.ndarray,
        saliency_map: np.ndarray,
        mask: Optional[np.ndarray] = None,
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        Applies the mask to the image.

        Parameters
        ----------
        img : np.ndarray
            The image with three dimensions in channel first order
            (channels, height, width).
        saliency_map : np.ndarray
            The saliency map with two dimensions, height and width.
            If the saliency map has three dimensions, the sum over
            the first dimension is calculated. The saliency map
            is then normalized.
        mask : Optional[np.ndarray], optional
            The mask to apply to the image, by default None.
            Can be used to apply a mask from a previous masking step.

        Returns
        -------
        Tuple[np.ndarray, np.ndarray]
            The masked image and the mask.
        """

        if len(saliency_map.shape) == 3:
            saliency_map = norm_map(np.sum(saliency_map, axis=0)).squeeze()
            saliency_map = saliency_map * 255
            saliency_map = saliency_map.astype(np.uint8)

        r = img.shape[1]/saliency_map.shape[0]
        assert r == img.shape[2]/saliency_map.shape[1], \
            "The image and the saliency map must have the same aspect ratio."

        mask = np.zeros((img.shape[1], img.shape[2])) if mask is None else mask
        steps = np.count_nonzero(mask == 1)
        steps = int((steps // r**2))

        # get the indices of the sorted saliency map in descending order
        sort_idx = np.unravel_index(
            np.argsort(saliency_map.ravel(), axis=None),
            saliency_map.shape
        )

        for i, j in zip(sort_idx[0][::-1][steps:], sort_idx[1][::-1][steps:]):
            mask[int(i*r):int((i+1)*r), int(j*r):int((j+1)*r)] = 1
            return img * np.repeat(mask[np.newaxis, :, :], 3, axis=0), mask

        return img, mask

    def _get_iauc(self) -> Tuple[float, np.ndarray, np.ndarray]:
        """
        Calculates the integrated IAUC for the current image.

        Returns
        -------
        Tuple[float, np.ndarray]
            The IAUC, the insertion scores.
        """
        img, map = self.maps[self.keys[self.current_idx]]
        masked_image = img
        mask = np.zeros((img.shape[1], img.shape[2]))
        outputs = []

        if len(map.shape) == 3:
            map = norm_map(np.sum(map, axis=0)).squeeze()
            map = map * 255
            map = map.astype(np.uint8)

        if self.verbose:
            pbar = tqdm(
                desc=f'IAUC - Masking image {self.current_idx}',
                total=map.size,
            )

        i = 0
        masked_imgs = []

        while i < map.size:
            masked_image, mask = self._get_masked_image(
                img=img,
                saliency_map=map,
                mask=mask,
            )
            norm_img = self.transform(np.moveaxis(masked_image, 0, -1))
            norm_img = norm_img.type(torch.FloatTensor).unsqueeze(0)
            masked_imgs.append(norm_img)

            if len(masked_imgs) >= 64 or mask.sum() <= mask.size:
                masked_imgs = torch.cat(masked_imgs, dim=0)
                prediction = self.model(masked_imgs.to(self.device))
                outputs.extend(prediction.detach().cpu().tolist())
                masked_imgs = []

            if self.verbose:
                pbar.update(1)
            i += 1

            if self.masking_steps is not None and i >= self.masking_steps:
                if len(masked_imgs) > 0:
                    masked_imgs = torch.cat(masked_imgs, dim=0)
                    prediction = self.model(masked_imgs.to(self.device))
                    outputs.extend(prediction.detach().cpu().tolist())
                break

        if self.verbose:
            pbar.close()
        outputs = np.array(outputs)
        outputs = F.softmax(torch.tensor(outputs), dim=1).numpy()

        inital_prediction = np.argmax(
            self.model(
                self.transform(
                    np.moveaxis(img, 0, -1)
                ).unsqueeze(0).to(self.device)
            ).detach().cpu().numpy()
        )

        insertion_score = outputs[:, inital_prediction]
        insertion_score /= insertion_score.max()

        return (
            np.trapz(insertion_score, dx=1/map.size),
            insertion_score,
        )


class Sparsity():

    @staticmethod
    def get_sparsity(
        saliency_map: np.ndarray,
    ) -> float:
        if len(saliency_map.shape) == 3:
            saliency_map = np.sum(saliency_map, axis=0)

        min_max_map = norm_map(saliency_map)
        return 1/np.mean(min_max_map)

    @staticmethod
    def get_all_sparsities(
        saliency_maps: np.ndarray,
    ) -> np.ndarray:
        return np.array([
            Sparsity.get_sparsity(saliency_map)
            for saliency_map in saliency_maps
        ])


class CorrelationBase():

    def __init__(
        self,
        model: torch.nn.Module,
        maps: Dict[int, Tuple[np.ndarray, np.ndarray]],
        device: torch.device,
        masking_steps: Optional[int] = None,
        verbose: bool = False,
    ) -> None:

        self.model = model
        self.maps = maps
        self.device = device
        self.masking_steps = masking_steps
        self.verbose = verbose

        if isinstance(self, IC):
            self.auc = IAUC(
                model=model,
                maps=maps,
                device=device,
                masking_steps=masking_steps,
                verbose=verbose,
            )
        elif isinstance(self, DC):
            self.auc = DAUC(
                model=model,
                maps=maps,
                device=device,
                masking_steps=masking_steps,
                verbose=verbose,
            )
        else:
            raise NotImplementedError(
                "The correlation method is not implemented."
            )

    def to_one_channel(self, img: np.ndarray) -> np.ndarray:
        if len(img.shape) == 3:
            img = np.sum(img, axis=0)
        return norm_map(img)


class IC(CorrelationBase):

    def __init__(
        self,
        model: torch.nn.Module,
        maps: Dict[int, Tuple[np.ndarray, np.ndarray]],
        device: torch.device,
        masking_steps: Optional[int] = None,
        verbose: bool = False,
        iauc_scores: Optional[Dict[str, List[Any]]] = None,
        n_samples: int = 100,
    ):
        super().__init__(
            model=model,
            maps=maps,
            device=device,
            masking_steps=masking_steps,
            verbose=verbose,
        )

        if iauc_scores is None:
            self.iauc_scores = {'iauc': [], 'idx': []}
            idx = 0
            while len(self.iauc_scores) < n_samples:
                try:
                    self.iauc_scores['iauc'].append(next(self.auc)[1])
                    self.iauc_scores['idx'].append(idx)
                    idx += 1
                except StopIteration:
                    break
            self.iauc_scores['iauc'] = np.array(self.iauc_scores)
        else:
            self.iauc_scores = iauc_scores

    def get_ic(self) -> float:
        variation = self.iauc_scores['iauc'][:, 1:] - self.iauc_scores['iauc'][:, :-1]
        variation = np.insert(variation, 0, 0, axis=1)

        saliency_scores = np.zeros_like(variation)
        for i, j in enumerate(self.iauc_scores['idx']):
            saliency_scores[i] = self.to_one_channel(self.maps[j][1]).flatten()

        saliency_scores = np.sort(saliency_scores, axis=1)
        correlation = [np.corrcoef(variation[i], saliency_scores[i])[0, 1] for i in range(variation.shape[0])]
        return np.array(correlation)


class DC(CorrelationBase):

    def __init__(
        self,
        model: torch.nn.Module,
        maps: Dict[int, Tuple[np.ndarray, np.ndarray]],
        device: torch.device,
        masking_steps: Optional[int] = None,
        verbose: bool = False,
        dauc_scores: Optional[Dict[str, List[Any]]] = None,
        n_samples: int = 100,
    ):
        super().__init__(
            model=model,
            maps=maps,
            device=device,
            masking_steps=masking_steps,
            verbose=verbose,
        )

        if dauc_scores is None:
            self.dauc_scores = {'dauc': [], 'idx': []}
            idx = 0
            while len(self.dauc_scores) < n_samples:
                try:
                    self.dauc_scores['dauc'].append(next(self.auc)[1])
                    self.dauc_scores['idx'].append(idx)
                    idx += 1
                except StopIteration:
                    break
            self.dauc_scores['dauc'] = np.array(self.dauc_scores['dauc'])
        else:
            self.dauc_scores = dauc_scores


    def get_dc(self):
        variation = np.diff(self.dauc_scores['dauc'], axis=1)
        variation = np.insert(variation, 0, variation.shape[1], axis=1)

        saliency_scores = np.zeros_like(variation)
        for i, j in enumerate(self.dauc_scores['idx']):
            saliency_scores[i] = self.to_one_channel(self.maps[j][1]).flatten()

        saliency_scores = np.sort(saliency_scores, axis=1)
        correlation = [np.corrcoef(variation[i], saliency_scores[i])[0, 1] for i in range(variation.shape[0])]
        return np.array(correlation)
